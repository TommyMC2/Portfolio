<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Content Moderation on the Internet</title>
    </head>
    <body>
        <h1>Introduction:</h1>
        <p>People have debated the best way to deal with certain types of speech since we developed
            language; since the creation of the internet, "content moderation" has gotten
            exponentially more complex. The internet has many people and groups with opposing views,
            goals, worldviews, and means to change things. Content moderation is a vital part of
            communication, and the platforms people use to communicate on the internet have never
            been able to implement a productive and scalable solution.</p>
        <h2>Past Attempts at Moderation</h2>
        <h3>Forums</h3>
        <p>In the early days of the internet, people congregated on forums, allowing them to
            communicate and log interactions between communities. These forums were the social media
            today(Reddit or Twitter) but on an infinitely smaller scale. They would usually be used
            for discussing a specific topic, like computers or games, but many were also used for
            general conversations. Moderation was rarely implemented in these forums due to the
            mindset that the internet was a new realm of endless possibilities and freedom. Due to
            this mindset, if a forum did adopt a moderation policy, it was usually narrow in the
            scope of its policies. Things like being off-topic concerning what the forum was for or
            illegal activities might have resulted in a ban.</p>
        <h3>Moderators</h3>
        <p>The way moderation policies would be enforced was through moderators. Moderators as a
            concept are still around today, but in the past were usually users of a forum who were
            trusted with certain privileges, which would allow them to help control how the forums
            would deal with policy violations. The actions a moderator could take primarily depended
            on the forum they occupied. However, they could usually ban and suspend accounts or
            delete, edit, and merge threads. Moderators were essentially the only way for a forum or
            platform to enforce policies in the past, but they were not the best or most effective
            solution. Moderators were people, and since most were unpaid, they could not constantly
            moderate, meaning sometimes a forum might have no one available to remove content or
            people.</p>
        <h2>Current Moderation</h2>
        <section>
            <p>Since the mass adoption of social media platforms like Twitter, Facebook, and Reddit
                by the average internet user, the amount of content that needs to be combed through
                has increased exponentially. Taken at face value, all this means is that more people
                have access to and utilize the internet, which is good. The downside is that with
                all these new people, moderation becomes much more challenging to manage because of
                the volume of content and the vastly different views on how far moderation should
                reach.</p>
            <h6>Statista</h6>
            <img src="images/statgraph.png" alt="Amount of social media users" />
            <p>One thing that has been a significant leap forward in mass content moderation is the
                large-scale use of automation. Most, if not all, major social media platforms use
                automated systems to detect and flag things that might go against the platforms'
                guidelines. Platforms like Twitter might check each word in a tweet to see if it
                contains anything against the policies of the platform, while YouTube has to look at
                not only the audio and what was said but the visuals and what might be on screen. </p>
            <p>One significant pitfall of automation is that the technology can be ineffective
                against people who wish to get their message out. Simple things like misspelled
                words, using a synonym, making minor changes to an image, or just implying something
                without outright saying it is all somewhat effective at bypassing ai moderation
                tools. AI is always getting better and is much cheaper than paying a large staff to
                manually look at and respond to reports of bad actors or content violations.
                Companies are trying to get past the shortcoming of AI and human staff by having
                them both work together. </p>
            <p>Companies are currently implementing AI by allowing it to do what it is best at
                making decisions on blatant violations while flagging content that may require a
                human review. This allows for the best of both worlds, where the AI can make
                decisions or categorize a large amount of content, and the people are freed up to
                take a better look at cases that are not as clear for it to make decisions on.
                However, working together allows manual moderators and AI to produce better outcomes
                than they could alone. Lingering problems remain, such as moderating teams needing
                more staff to handle the increasing content and the mental health toll content
                moderation can have.</p>
        </section>
        <section>
            <h3>Moderators at Large Companies</h3>
            <p>Moderators are dealing with an ever-increasing workload that leaves them overworked
                and exposed to some of the most violent content on the internet. The amount of time
                spent on checking individual posts ranges from a few seconds to no more than 30
                seconds to keep up with the ever-expanding backlog. This pace can easily lead to
                burnout because it requires employees to memorize specific company policies since
                there is no real-time to check them within the time provided.</p>
            <p>When most people think about content moderation, they picture a Twitter post or a
                nasty comment, but some of the most harmful content you come across in the field is
                violent videos. The Verge reported the story of a training exercise used on future
                Facebook moderators, a video of a man being violently stabbed to death while begging
                for his life is played. The person recounting the story was supposed to explain how
                the video violated Facebook's rules but left the room sobbing and unable to breathe
                due to what she had witnessed. It is not a fun thought, but you do not see things
                like this online much because someone else spends their days scrubbing content like
                this from platforms. Being exposed to violent images like this so regularly leaves
                people with lingering mental health problems and can make it more challenging to
                interact with people socially.</p>
            <p>"...in their own words, a lot of the workers would claim that they didn't have any
                ill effects. And I would take that at face value, and then we would continue talking
                and engaging, and they would say things to me, like, "You know, since I took this
                job, I've really been drinking a lot. I just come home at night, I don't really want
                to talk to anyone." Or, "I find myself avoiding social situations." <a
                    href="https://www.newyorker.com/news/q-and-a/the-underworld-of-online-content-moderation"
                    >The Underworld of Online Content Moderation</a></p>
            <p>What might be some future alternatives to modern moderation? The future of content
                moderation is automated. The better AI gets, the more companies will allow it to do
                more and more. This path might have pros, like no more human moderators being forced
                to look at endless amounts of violence and creating a faster, more efficient way of
                enforcing policy violations. Conversely, the cons are a complete lack of community
                input. If AI takes over the moderation job, the communities that built these
                platforms will lose their ability to effect change.</p>
            <p>The tech industry is giving up on community moderation, with one of the only
                significant platforms that still have it being Reddit. Reddit is unique in that
                instead of a free-roam platform like Twitter, Reddit returns to the early internet's
                use of forums. Most users on Reddit congregate on specific forums, which creates
                many smaller communities that can create policies for how people can act, along with
                building their moderation teams, which are tasked with removing content and members.
                Each subreddit may be able to implement its own rules, but everyone must still
                adhere to the platform's <a href="https://www.redditinc.com/policies/content-policy"
                    >content policy</a>.</p>
            <p>Community moderation has the same downfalls as before: they cannot be active all the
                time, and if the community is small, it might be hard to find anyone willing to do
                the job. Reddits' solution is allowing subreddits to use their "AutoModerator." The
                AutoModerator is not an AI but functions as a sort of early reporting system; it can
                check to make sure rules are followed, remove posts and comments with banned words,
                and, most importantly, let the moderation team know if a post is getting a certain
                amount of reports.</p>
            <p>While not a perfect solution, it has some of the same shortcomings as other forms of
                moderation, like not being as efficient as it needs to be or the violence moderators
                are still exposed to. Community moderation has many benefits companies not needing
                to pay a team to moderate the website, have more flexibility when it comes to what
                is and is not allowed for different communities, and allow communities to help shape
                the forums they occupy.</p>
        </section>
        <section>
            <h2>Conclusion</h2>
            <p>Content moderation has and will continue to evolve into the future. New things will
                be tried, and useless things will be left behind, but soon, many major companies
                will begin to consolidate content moderation into AI tools. Leaving the human
                element out of moderating will make it harder for all of us to influence the
                internet we occupy, allowing big social media companies to dictate how content
                should be moderated.</p>
        </section>
    </body>
</html>
